Model alignment is about making sure a model, like ChatGPT, not only gives correct answers but also answers that people find valuable and acceptable. This includes avoiding mistakes and biases that could make the model’s responses less helpful or inappropriate.
Large language models (LLMs) like GPT-3 had been in development for years, but GPT-3.5, the model behind ChatGPT, brought a significant shift in terms of performance and excitement. The key difference between GPT-3 and GPT-3.5 was the use of RLHF.
RLHF Overview: RLHF is a technique that allows for fine-tuning models to align their responses with human expectations, focusing not just on accuracy but on minimizing biases. The process involves:
Supervised Fine-Tuning: Human labelers provide demonstrations and comparisons to guide the model's responses.
Reinforcement Learning: The model’s responses are further refined using human feedback, which can be direct evaluation or proxy measures. A reward model scores the outputs based on how well they align with human preferences.Humans show the model examples of good and bad responses so it can learn the right way to answer.
Effectiveness of RLHF: This technique proved to be highly efficient in controlling model outputs and became a key factor in improving models to compete with GPT-3.5. It was initially thought that any model aiming to compete with GPT-3.5 would need to implement RLHF. In other words, After the model learns, humans give feedback on the responses, and the model adjusts to improve based on that feedback. This helps the model get better at understanding what humans want and avoid giving biased or inaccurate answers.

